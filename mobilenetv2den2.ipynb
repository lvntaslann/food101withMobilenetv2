{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0600f5b-2928-4a1b-918b-747fdb26b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import stat\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import random\n",
    "import cv2\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import defaultdict\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import load_model\n",
    "from shutil import copy\n",
    "from shutil import copytree, rmtree\n",
    "import tensorflow.keras.backend \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9762f676-a352-43c8-91fc-0342537753a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to load train-test files.\n",
    "def load_train_test_data(path_to_train_imgs, path_to_test_imgs):\n",
    "    X_train, y_train = load_images(path_to_train_imgs)\n",
    "    X_test, y_test = load_images(path_to_test_imgs)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35317f19-a8cc-4267-a119-90025039ce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75750 images belonging to 101 classes.\n",
      "Found 25250 images belonging to 101 classes.\n",
      "Epoch 1/20\n",
      "2367/2367 [==============================] - 699s 293ms/step - loss: 2.3663 - accuracy: 0.4382 - val_loss: 2.7993 - val_accuracy: 0.4015\n",
      "Epoch 2/20\n",
      "2367/2367 [==============================] - 655s 277ms/step - loss: 1.5405 - accuracy: 0.6121 - val_loss: 2.4705 - val_accuracy: 0.4388\n",
      "Epoch 3/20\n",
      "2367/2367 [==============================] - 630s 266ms/step - loss: 1.2847 - accuracy: 0.6720 - val_loss: 1.8798 - val_accuracy: 0.5565\n",
      "Epoch 4/20\n",
      "2367/2367 [==============================] - 631s 266ms/step - loss: 1.1439 - accuracy: 0.7026 - val_loss: 2.0717 - val_accuracy: 0.5295\n",
      "Epoch 5/20\n",
      "2367/2367 [==============================] - 628s 265ms/step - loss: 1.0256 - accuracy: 0.7302 - val_loss: 1.9365 - val_accuracy: 0.5680\n",
      "Epoch 6/20\n",
      "2367/2367 [==============================] - 629s 266ms/step - loss: 0.9374 - accuracy: 0.7511 - val_loss: 1.5258 - val_accuracy: 0.6282\n",
      "Epoch 7/20\n",
      "2367/2367 [==============================] - 633s 267ms/step - loss: 0.8744 - accuracy: 0.7665 - val_loss: 1.6792 - val_accuracy: 0.6221\n",
      "Epoch 8/20\n",
      "2367/2367 [==============================] - 628s 265ms/step - loss: 0.8097 - accuracy: 0.7811 - val_loss: 1.4272 - val_accuracy: 0.6588\n",
      "Epoch 9/20\n",
      "2367/2367 [==============================] - 626s 264ms/step - loss: 0.7544 - accuracy: 0.7943 - val_loss: 1.5528 - val_accuracy: 0.6327\n",
      "Epoch 10/20\n",
      "2367/2367 [==============================] - 629s 266ms/step - loss: 0.7017 - accuracy: 0.8068 - val_loss: 1.4812 - val_accuracy: 0.6613\n",
      "Epoch 11/20\n",
      "2367/2367 [==============================] - 627s 265ms/step - loss: 0.6593 - accuracy: 0.8172 - val_loss: 1.3921 - val_accuracy: 0.6759\n",
      "Epoch 12/20\n",
      "2367/2367 [==============================] - 624s 264ms/step - loss: 0.6204 - accuracy: 0.8283 - val_loss: 1.6036 - val_accuracy: 0.6436\n",
      "Epoch 13/20\n",
      "2367/2367 [==============================] - 631s 267ms/step - loss: 0.5923 - accuracy: 0.8350 - val_loss: 1.3324 - val_accuracy: 0.6940\n",
      "Epoch 14/20\n",
      "2367/2367 [==============================] - 640s 270ms/step - loss: 0.5607 - accuracy: 0.8412 - val_loss: 1.3299 - val_accuracy: 0.7023\n",
      "Epoch 15/20\n",
      "2367/2367 [==============================] - 641s 271ms/step - loss: 0.5233 - accuracy: 0.8519 - val_loss: 1.4158 - val_accuracy: 0.6777\n",
      "Epoch 16/20\n",
      "2367/2367 [==============================] - 638s 270ms/step - loss: 0.5024 - accuracy: 0.8579 - val_loss: 1.3394 - val_accuracy: 0.6815\n",
      "Epoch 17/20\n",
      "2367/2367 [==============================] - 640s 271ms/step - loss: 0.4664 - accuracy: 0.8674 - val_loss: 1.6939 - val_accuracy: 0.6399\n",
      "Epoch 18/20\n",
      "2367/2367 [==============================] - 636s 269ms/step - loss: 0.4517 - accuracy: 0.8702 - val_loss: 1.3449 - val_accuracy: 0.7203\n",
      "Epoch 19/20\n",
      "2367/2367 [==============================] - 627s 265ms/step - loss: 0.4229 - accuracy: 0.8765 - val_loss: 1.4000 - val_accuracy: 0.7025\n",
      "Epoch 20/20\n",
      "2367/2367 [==============================] - 627s 265ms/step - loss: 0.4092 - accuracy: 0.8806 - val_loss: 1.4953 - val_accuracy: 0.6941\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Clearing the session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Defining parameters\n",
    "n_classes = 101\n",
    "img_width, img_height = 224, 224\n",
    "train_data_dir = r\"C:\\Users\\kurt_\\Desktop\\food-101\\train\"\n",
    "validation_data_dir = r\"C:\\Users\\kurt_\\Desktop\\food-101\\test\"\n",
    "nb_train_samples = 75750\n",
    "nb_validation_samples = 25250\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# MobileNetV2 model\n",
    "mbv2 = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "x = mbv2.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=mbv2.input, outputs=predictions)\n",
    "model.compile(optimizer=Adamax(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_101class_mobilenetv2den2.hdf5', save_best_only=True)\n",
    "csv_logger = CSVLogger('history_101class_mobilenetv2den2.log')\n",
    "\n",
    "# Training the model\n",
    "history_101class_mobilenetv2den2 = model.fit(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, csv_logger])\n",
    "\n",
    "# Saving the trained model\n",
    "model.save('model_trained_101class_mobilenetv2den2.hdf5')\n",
    "print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe39d5-8f0c-4f67-b3d3-44425fcc5e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
