{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ab533b-c649-465c-8daa-733a7d4ddda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import stat\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import random\n",
    "import cv2\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import defaultdict\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import load_model\n",
    "from shutil import copy\n",
    "from shutil import copytree, rmtree\n",
    "import tensorflow.keras.backend \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5022576c-e7d1-4c91-83cf-e9d28b79db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_test_split(image_dir, target_dir, train_ratio=0.75):\n",
    "    # Eğer hedef dizin yoksa oluştur\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "    \n",
    "    train_dir = os.path.join(target_dir, 'train')\n",
    "    test_dir = os.path.join(target_dir, 'test')\n",
    "    \n",
    "    # Eğer eğitim ve test dizinleri yoksa oluştur\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "    \n",
    "    # Görüntü dosyalarını ve sınıfları yükle\n",
    "    all_classes = os.listdir(image_dir)\n",
    "    for class_name in all_classes:\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        all_images = os.listdir(class_dir)\n",
    "        num_images = len(all_images)\n",
    "        \n",
    "        # Eğitim ve test örneklerini rastgele seç\n",
    "        num_train_samples = int(train_ratio * num_images)\n",
    "        train_samples_selected = random.sample(all_images, num_train_samples)\n",
    "        test_samples_selected = [img for img in all_images if img not in train_samples_selected]\n",
    "        \n",
    "        # Eğitim örneklerini kopyala\n",
    "        for train_img in train_samples_selected:\n",
    "            src_train = os.path.join(class_dir, train_img)\n",
    "            dest_train = os.path.join(train_dir, class_name, train_img)\n",
    "            if not os.path.exists(os.path.join(train_dir, class_name)):\n",
    "                os.makedirs(os.path.join(train_dir, class_name))\n",
    "            shutil.copy(src_train, dest_train)\n",
    "        \n",
    "        # Test örneklerini kopyala\n",
    "        for test_img in test_samples_selected:\n",
    "            src_test = os.path.join(class_dir, test_img)\n",
    "            dest_test = os.path.join(test_dir, class_name, test_img)\n",
    "            if not os.path.exists(os.path.join(test_dir, class_name)):\n",
    "                os.makedirs(os.path.join(test_dir, class_name))\n",
    "            shutil.copy(src_test, dest_test)\n",
    "\n",
    "# Görüntü dizini\n",
    "image_dir = r\"C:\\Users\\kurt_\\Desktop\\food-101\\images\"\n",
    "# Hedef dizin\n",
    "target_dir = r\"C:\\Users\\kurt_\\Desktop\\food-101\"\n",
    "\n",
    "# Eğitim ve test örneği oranı\n",
    "train_ratio = 0.75\n",
    "\n",
    "# Train ve test setlerini oluştur\n",
    "gen_train_test_split(image_dir, target_dir, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38d5cd3-2da2-4c37-8c93-6a75af60677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test folders already exist.\n",
      "101 101\n"
     ]
    }
   ],
   "source": [
    "# Method to load train-test files.\n",
    "def load_train_test_data(path_to_train_imgs, path_to_test_imgs):\n",
    "    X_train, y_train = load_images(path_to_train_imgs)\n",
    "    X_test, y_test = load_images(path_to_test_imgs)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Generate train-test files. \n",
    "if not os.path.isdir(\"C:/Users/kurt_/Desktop/food-101/test\") and not os.path.isdir(\"C:/Users/kurt_/Desktop/food-101/train\"):\n",
    "    gen_train_test_split()  \n",
    "    len_train = len(os.listdir(\"C:/Users/kurt_/Desktop/food-101/train\"))\n",
    "    len_test = len(os.listdir(\"C:/Users/kurt_/Desktop/food-101/test\"))\n",
    "    print(len_train, len_test)\n",
    "else:\n",
    "    print('train and test folders already exist.')\n",
    "    len_train = len(os.listdir(\"C:/Users/kurt_/Desktop/food-101/train\"))\n",
    "    len_test = len(os.listdir(\"C:/Users/kurt_/Desktop/food-101/test\"))\n",
    "    print(len_train, len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2925342e-8656-40f1-81f8-08877a23073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75750 images belonging to 101 classes.\n",
      "Found 25250 images belonging to 101 classes.\n",
      "Epoch 1/30\n",
      "1515/1515 [==============================] - 1509s 994ms/step - loss: 2.2918 - accuracy: 0.4548 - val_loss: 3.2634 - val_accuracy: 0.3605\n",
      "Epoch 2/30\n",
      "1515/1515 [==============================] - 906s 598ms/step - loss: 1.4638 - accuracy: 0.6317 - val_loss: 2.8758 - val_accuracy: 0.4225\n",
      "Epoch 3/30\n",
      "1515/1515 [==============================] - 921s 608ms/step - loss: 1.2201 - accuracy: 0.6861 - val_loss: 2.4766 - val_accuracy: 0.4972\n",
      "Epoch 4/30\n",
      "1515/1515 [==============================] - 858s 566ms/step - loss: 1.0656 - accuracy: 0.7214 - val_loss: 1.7110 - val_accuracy: 0.5890\n",
      "Epoch 5/30\n",
      "1515/1515 [==============================] - 859s 567ms/step - loss: 0.9563 - accuracy: 0.7476 - val_loss: 1.9736 - val_accuracy: 0.5511\n",
      "Epoch 6/30\n",
      "1515/1515 [==============================] - 854s 564ms/step - loss: 0.8743 - accuracy: 0.7668 - val_loss: 1.6058 - val_accuracy: 0.6267\n",
      "Epoch 7/30\n",
      "1515/1515 [==============================] - 896s 592ms/step - loss: 0.7973 - accuracy: 0.7851 - val_loss: 1.5540 - val_accuracy: 0.6325\n",
      "Epoch 8/30\n",
      "1515/1515 [==============================] - 837s 552ms/step - loss: 0.7375 - accuracy: 0.8010 - val_loss: 1.4680 - val_accuracy: 0.6596\n",
      "Epoch 9/30\n",
      "1515/1515 [==============================] - 809s 534ms/step - loss: 0.6858 - accuracy: 0.8115 - val_loss: 1.5073 - val_accuracy: 0.6521\n",
      "Epoch 10/30\n",
      "1515/1515 [==============================] - 809s 534ms/step - loss: 0.6357 - accuracy: 0.8255 - val_loss: 1.4047 - val_accuracy: 0.6726\n",
      "Epoch 11/30\n",
      "1515/1515 [==============================] - 878s 579ms/step - loss: 0.5928 - accuracy: 0.8357 - val_loss: 1.6578 - val_accuracy: 0.6142\n",
      "Epoch 12/30\n",
      "1515/1515 [==============================] - 872s 576ms/step - loss: 0.5591 - accuracy: 0.8437 - val_loss: 1.5648 - val_accuracy: 0.6419\n",
      "Epoch 13/30\n",
      "1515/1515 [==============================] - 822s 542ms/step - loss: 0.5210 - accuracy: 0.8529 - val_loss: 1.3822 - val_accuracy: 0.6882\n",
      "Epoch 14/30\n",
      "1515/1515 [==============================] - 810s 534ms/step - loss: 0.4872 - accuracy: 0.8609 - val_loss: 1.7385 - val_accuracy: 0.6241\n",
      "Epoch 15/30\n",
      "1515/1515 [==============================] - 855s 564ms/step - loss: 0.4574 - accuracy: 0.8699 - val_loss: 1.5200 - val_accuracy: 0.6773\n",
      "Epoch 16/30\n",
      "1515/1515 [==============================] - 887s 585ms/step - loss: 0.4294 - accuracy: 0.8766 - val_loss: 1.5273 - val_accuracy: 0.6748\n",
      "Epoch 17/30\n",
      "1515/1515 [==============================] - 828s 546ms/step - loss: 0.4036 - accuracy: 0.8831 - val_loss: 1.6975 - val_accuracy: 0.6425\n",
      "Epoch 18/30\n",
      "1515/1515 [==============================] - 804s 531ms/step - loss: 0.3844 - accuracy: 0.8876 - val_loss: 1.4761 - val_accuracy: 0.6777\n",
      "Epoch 19/30\n",
      "1515/1515 [==============================] - 804s 530ms/step - loss: 0.3651 - accuracy: 0.8931 - val_loss: 1.5479 - val_accuracy: 0.6677\n",
      "Epoch 20/30\n",
      "1515/1515 [==============================] - 889s 587ms/step - loss: 0.3493 - accuracy: 0.8968 - val_loss: 1.6796 - val_accuracy: 0.6567\n",
      "Epoch 21/30\n",
      "1515/1515 [==============================] - 829s 547ms/step - loss: 0.3235 - accuracy: 0.9040 - val_loss: 1.5719 - val_accuracy: 0.6773\n",
      "Epoch 22/30\n",
      "1515/1515 [==============================] - 831s 549ms/step - loss: 0.3082 - accuracy: 0.9082 - val_loss: 1.8375 - val_accuracy: 0.6434\n",
      "Epoch 23/30\n",
      "1515/1515 [==============================] - 890s 587ms/step - loss: 0.2950 - accuracy: 0.9110 - val_loss: 1.5228 - val_accuracy: 0.6720\n",
      "Epoch 24/30\n",
      "1515/1515 [==============================] - 891s 588ms/step - loss: 0.2814 - accuracy: 0.9163 - val_loss: 1.3664 - val_accuracy: 0.6958\n",
      "Epoch 25/30\n",
      "1515/1515 [==============================] - 851s 561ms/step - loss: 0.2627 - accuracy: 0.9218 - val_loss: 1.5195 - val_accuracy: 0.6994\n",
      "Epoch 26/30\n",
      "1515/1515 [==============================] - 844s 557ms/step - loss: 0.2555 - accuracy: 0.9227 - val_loss: 1.4970 - val_accuracy: 0.6943\n",
      "Epoch 27/30\n",
      "1515/1515 [==============================] - 829s 547ms/step - loss: 0.2513 - accuracy: 0.9244 - val_loss: 1.5121 - val_accuracy: 0.6983\n",
      "Epoch 28/30\n",
      "1515/1515 [==============================] - 873s 576ms/step - loss: 0.2328 - accuracy: 0.9289 - val_loss: 1.5988 - val_accuracy: 0.6848\n",
      "Epoch 29/30\n",
      "1515/1515 [==============================] - 872s 575ms/step - loss: 0.2221 - accuracy: 0.9322 - val_loss: 1.6899 - val_accuracy: 0.6813\n",
      "Epoch 30/30\n",
      "1515/1515 [==============================] - 840s 555ms/step - loss: 0.2169 - accuracy: 0.9335 - val_loss: 1.4645 - val_accuracy: 0.7081\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Clearing the session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Defining parameters\n",
    "n_classes = 101\n",
    "img_width, img_height = 224, 224\n",
    "train_data_dir = r\"C:\\Users\\kurt_\\Desktop\\food-101\\train\"\n",
    "validation_data_dir = r\"C:\\Users\\kurt_\\Desktop\\food-101\\test\"\n",
    "nb_train_samples = 75750\n",
    "nb_validation_samples = 25250\n",
    "batch_size = 50\n",
    "epochs = 30\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# MobileNetV2 model\n",
    "mbv2 = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "x = mbv2.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=mbv2.input, outputs=predictions)\n",
    "model.compile(optimizer=Adamax(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_101class_mobilenetv2.hdf5', save_best_only=True)\n",
    "csv_logger = CSVLogger('history_101class_mobilenetv2.log') # Fixed this line\n",
    "\n",
    "# Training the model\n",
    "history_101class_mobilenetv2 = model.fit(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, csv_logger])\n",
    "\n",
    "# Saving the trained model\n",
    "model.save('model_trained_101class_mobilenetv2.hdf5')\n",
    "print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0214feb-fad0-436e-bc07-b4d762fe275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\kurt_\\best_model_101class_mobilenetv2.hdf5\"\n",
    "test_data_path = r\"C:\\Users\\kurt_\\Desktop\\food-101\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3e58901-cf50-4585-a3f8-ab7e6adfbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performans metriklerini hesaplamak için fonksiyon tanımla\n",
    "def calculate_metrics(model, test_gen):\n",
    "    # Modelden tahminler yap\n",
    "    predictions = model.predict(test_gen)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Gerçek sınıfları al\n",
    "    true_classes = test_gen.classes\n",
    "    \n",
    "    # Sınıf etiketlerini al\n",
    "    class_labels = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # Doğruluk, hassasiyet, geri çağırma ve F1 puanını hesapla\n",
    "    accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "    precision = precision_score(true_classes, predicted_classes, average='weighted')\n",
    "    recall = recall_score(true_classes, predicted_classes, average='weighted')\n",
    "    f1 = f1_score(true_classes, predicted_classes, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06e9893d-eb74-41e1-a01d-d9fb3e437b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25250 images belonging to 101 classes.\n",
      "1579/1579 [==============================] - 85s 53ms/step\n",
      "Accuracy: 0.696\n",
      "Precision: 0.7285602710387407\n",
      "Recall: 0.696\n",
      "F1 Score: 0.6987808495076148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Boyutları tanımla\n",
    "height, width = 224, 224\n",
    "\n",
    "# Batch boyutunu tanımla\n",
    "batch_size = 16\n",
    "\n",
    "# Test veri üreteci oluştur\n",
    "test_data_gen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_gen = test_data_gen.flow_from_directory(test_data_path, target_size=(height, width), batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
    "\n",
    "# Modeli yükle\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# Performans metriklerini hesapla\n",
    "accuracy, precision, recall, f1_score = calculate_metrics(model, test_gen)\n",
    "\n",
    "# Hesaplanan metrikleri yazdır\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a62fac-8449-4ed1-b146-e71a21be6faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
